{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "Take in the famous MNIST data set of handwritten digits and create a Deep Learning model using TensforFlow to correctly classify them.\n",
    "\n",
    "## Steps:\n",
    "I will take the following steps to acheive my goal:\n",
    "1. Set model parameters\n",
    "2. Set placeholders, weights, and biases\n",
    "3. Define perceptron, cost, and optimizer\n",
    "4. Run the model\n",
    "5. Test the model\n",
    "6. Refine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Setting the parameters\n",
    "Here are the initial paramters that I am setting for the first run through this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "n_samples = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Placeholders, weights, and biases\n",
    "These are the weights and biases that will be applied to the model as the input travels through the hidden layers and to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float',[None,n_input])\n",
    "y = tf.placeholder('float',[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2,n_classes])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Define perceptron, cost, and optimizer\n",
    "I have chosen to use two hidden layers in this deep learning model. Adding more hidden layers would increase the accuracy of the model, however with this dataset 2 hidden layers is enough to achieve a good accuracy. \n",
    "\n",
    "My activation fucntion will be RELU and the optimizer will be the Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x,weights,biases):\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2,weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "pred = multilayer_perceptron(x,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost and optimiser \n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Running the session\n",
    "Now it's time to run our model. The two for loops will loop through each epoch and batch respectively and then print out the cost for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=165.4091\n",
      "Epoch: 2 cost=42.0048\n",
      "Epoch: 3 cost=27.1312\n",
      "Epoch: 4 cost=19.0818\n",
      "Epoch: 5 cost=14.2210\n",
      "Epoch: 6 cost=10.6880\n",
      "Epoch: 7 cost=8.0346\n",
      "Epoch: 8 cost=6.0636\n",
      "Epoch: 9 cost=4.4497\n",
      "Epoch: 10 cost=3.4474\n",
      "Epoch: 11 cost=2.5600\n",
      "Epoch: 12 cost=1.9112\n",
      "Epoch: 13 cost=1.3684\n",
      "Epoch: 14 cost=1.2052\n",
      "Epoch: 15 cost=0.8536\n",
      "Model has completed 15 Epochs of Training\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.0 Testing the model\n",
    "Now that I have ran through the first iteration of our model, it is time to evalulate the accuracy of what I created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "correct_predictions = tf.cast(correct_predictions, \"float\")\n",
    "accuracy = tf.reduce_mean(correct_predictions)\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Improving the model\n",
    "Now it is time to improve our model by tweaking the parameters. The parameters that I set at the beginning are what I will be adjusting. \n",
    "\n",
    "Deep learning in a fast moving field, and the learning rate finder has been a topic of discussion recently. The idea being that you can create a model that gradually increases the learning rate to find the optimum for a number of batch sizes. \n",
    "\n",
    "Here however I will not be using this model and using an intuition based approach where I assume that decreasing the batch size and decrasing the learning rate will reduce my loss and increase accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=118.0878\n",
      "Epoch: 2 cost=29.2770\n",
      "Epoch: 3 cost=17.2393\n",
      "Epoch: 4 cost=11.1376\n",
      "Epoch: 5 cost=7.4225\n",
      "Epoch: 6 cost=5.1362\n",
      "Epoch: 7 cost=3.5494\n",
      "Epoch: 8 cost=2.5115\n",
      "Epoch: 9 cost=1.7533\n",
      "Epoch: 10 cost=1.3747\n",
      "Epoch: 11 cost=1.0053\n",
      "Epoch: 12 cost=0.9000\n",
      "Epoch: 13 cost=0.7001\n",
      "Epoch: 14 cost=0.7300\n",
      "Epoch: 15 cost=0.5834\n",
      "Model has completed 15 Epochs of Training\n",
      "Accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "# New parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 15\n",
    "batch_size = 50\n",
    "\n",
    "# Run the model again\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))\n",
    "\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "correct_predictions = tf.cast(correct_predictions, \"float\")\n",
    "accuracy = tf.reduce_mean(correct_predictions)\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing my epoch rate I can also increase the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=119.1054\n",
      "Epoch: 2 cost=31.7345\n",
      "Epoch: 3 cost=18.4623\n",
      "Epoch: 4 cost=12.0373\n",
      "Epoch: 5 cost=7.9507\n",
      "Epoch: 6 cost=5.6495\n",
      "Epoch: 7 cost=3.8142\n",
      "Epoch: 8 cost=2.7813\n",
      "Epoch: 9 cost=2.0712\n",
      "Epoch: 10 cost=1.5027\n",
      "Epoch: 11 cost=1.1495\n",
      "Epoch: 12 cost=0.9879\n",
      "Epoch: 13 cost=0.9631\n",
      "Epoch: 14 cost=0.8315\n",
      "Epoch: 15 cost=0.5799\n",
      "Epoch: 16 cost=0.6213\n",
      "Epoch: 17 cost=0.5586\n",
      "Epoch: 18 cost=0.5539\n",
      "Epoch: 19 cost=0.4693\n",
      "Epoch: 20 cost=0.4628\n",
      "Epoch: 21 cost=0.4194\n",
      "Epoch: 22 cost=0.4373\n",
      "Epoch: 23 cost=0.3462\n",
      "Epoch: 24 cost=0.4385\n",
      "Epoch: 25 cost=0.3414\n",
      "Model has completed 25 Epochs of Training\n",
      "Accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "# New parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 25\n",
    "batch_size = 50\n",
    "\n",
    "# Run the model again\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(n_samples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={:.4f}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Model has completed {} Epochs of Training\".format(training_epochs))\n",
    "\n",
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "correct_predictions = tf.cast(correct_predictions, \"float\")\n",
    "accuracy = tf.reduce_mean(correct_predictions)\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Through some tweaking of the initial parameters I was able to increase accuracy from 94% to 96%. The way I did this was:\n",
    "- Decreasing the learning rate and batch size\n",
    "- Increasing the epoch count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
