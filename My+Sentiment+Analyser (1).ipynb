{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "My goal is to load a dataset containing reviews and create a sentiment analyser model to determine whether those reviews are negative or positive. I will then analyse the model to see how I can improve it.\n",
    "\n",
    "## Process:\n",
    "When I undertake a machine learning project, I always lay out my process to improve my workflow and ability to explain the project findings. For this project, my workflow process will be:\n",
    "1. Load in the data set\n",
    "2. Split the data into training and testing\n",
    "3. Create model\n",
    "4. Train and test the model\n",
    "5. Analyse the results \n",
    "6. Make improvements\n",
    "7. Apply to test data set and export\n",
    "8. Real-world Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Importing the data\n",
    "The dataset I will be using in this project is from The University of Michigan who asked their students to review a list of films. I have two datasets here, the training and the testing data. The training data contains a binary feature that denotes if the review was positive or negative. The testing data does not have this feature.\n",
    "\n",
    "I will be importing the csv files as data frames, making sure I separate out the different features by tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('trainingdata.csv',header=None,sep='\\t',names=['target','text'])\n",
    "test = pd.read_csv('testdata.csv',sep='\\t',header=None,names=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does my data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1            The Da Vinci Code book is just awesome.\n",
       "1       1  this was the first clive cussler i've ever rea...\n",
       "2       1                   i liked the Da Vinci Code a lot.\n",
       "3       1                   i liked the Da Vinci Code a lot.\n",
       "4       1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Splitting the data\n",
    "I will be splitting the 'training' dataset into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train.drop('target',axis=1)\n",
    "y = train['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Create the model\n",
    "To create this model I will be using the pipeline feature from sklearn. Within this pipeline I will include a CountVectorizer to clean my input text (getting rid of stopwords and punctuation) and convert it to vector format, TfidfTransformer to add weights to important words, and a MultinomialNB to classify the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning the text\n",
    "To improve the accuracy of the model I will first create a function that will remove all the stopwords and punctuation from the reviews. Stopwords are words such as: the, when, if, that etc. these words don't tell us anything about the text so they will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \n",
    "    nopunc = [char for char in string if char not in punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return [word.lower() for word in nopunc if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take in a string, remove the punctuation, stopwords and then return the cleaned string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Creating the pipeline\n",
    "Now it's time to create my pipeline. Within this pipeline I will include 3 elements:\n",
    "1. CountVectorizer.\n",
    "This will convert the strings into a matrix of tokens, whilst also using the function I defined in 3.1 to clean the string.\n",
    "2. TfidfTransformer.\n",
    "This will transform the matrix of tokens into a normalized tf-idf representation, effectively adding weights to words if they are more important in the corpus.\n",
    "3. MultinomialNB.\n",
    "This is my classifier for my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Train and test my model\n",
    "Now that I created my pipeline, it is time to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=<function clean_text at 0x1a0c974840>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train.values,y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_one = pipeline.predict(x_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Analyse the results\n",
    "Let's see how our model has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 553  434]\n",
      " [  49 1247]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.56      0.70       987\n",
      "          1       0.74      0.96      0.84      1296\n",
      "\n",
      "avg / total       0.82      0.79      0.78      2283\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 78.8436%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test.values,predictions_one))\n",
    "print('\\n')\n",
    "print(classification_report(y_test.values,predictions_one))\n",
    "print('\\n')\n",
    "print(\"Accuracy: {:.4%}\".format(np.mean(predictions_one == y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first pass at this model, it's a good set of results. It would appear that our model struggled with precision of positive reviews and the recall of negative reviews. Let's see if we can improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Improving the model\n",
    "When using machine learning it is important to assess and improve the model. To find areas of improvement I will try the following:\n",
    "1. Use GridSearchCV to find optimal parameters\n",
    "2. Use a different classifier in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 GridSearchCV with original model\n",
    "To see if I can increase the accuracy of my original model pipeline I will use GridSearchCV to find the optimal parameters I pass through. \n",
    "\n",
    "I need to tell GridSearchCV which parameters to test first. Below are the parameters that I'm asking it to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1,1),(1,2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'classifier__alpha': (1,1e-1,1e-2,1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 80.2min finished\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(pipeline, parameters, verbose=1)\n",
    "gs_clf = gs_clf.fit(x_train.values,y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__alpha': 0.1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best parameters for my pipeline are the following\n",
    "\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_two = gs_clf.predict(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 552  435]\n",
      " [  52 1244]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.56      0.69       987\n",
      "          1       0.74      0.96      0.84      1296\n",
      "\n",
      "avg / total       0.82      0.79      0.77      2283\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 78.6684%\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test.values,predictions_two))\n",
    "print('\\n')\n",
    "print(classification_report(y_test.values,predictions_two))\n",
    "print('\\n')\n",
    "print(\"Accuracy: {:.4%}\".format(np.mean(predictions_two == y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parameters that it suggested I change was the classifier alpha from 1.0 to 0.1. The accuracy was not increased.\n",
    "\n",
    "### 6.2 Using BernoulliNB\n",
    "\n",
    "Next I will try to use a different classifier, BernoulliNB. From my research I believe that this should improve my results due to it working well on binary features, such as the word vectors in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(analyzer=clean_text)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('classifier',BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer=<function clean_text at 0x1a0c974840>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,..._idf=True)), ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train.values,y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to see if my change of classifier will improve our accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_three = pipeline.predict(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 857  130]\n",
      " [ 254 1042]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.87      0.82       987\n",
      "          1       0.89      0.80      0.84      1296\n",
      "\n",
      "avg / total       0.84      0.83      0.83      2283\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 83.1800%\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test.values,predictions_three))\n",
    "print('\\n')\n",
    "print(classification_report(y_test.values,predictions_three))\n",
    "print('\\n')\n",
    "print(\"Accuracy: {:.4%}\".format(np.mean(predictions_three == y_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy was increased by a few percentage points. What is also interesting about these results is that our model was much more balanced in terms of predicting and recalling both positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Final accuracy results\n",
    "I have now created three models. The first being my original model, the second after adjusting parameters, and the third after changing my classifier. \n",
    "\n",
    "The accuracy results for those three models are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BernoulliNB': 0.83180026281208941,\n",
       " 'Multinomial NB after GridSearchCV': 0.7866841874726237,\n",
       " 'MultinomialNB': 0.78843626806833111}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_results = {'MultinomialNB': (np.mean(predictions_one == y_test.values)), 'Multinomial NB after GridSearchCV': (np.mean(predictions_two == y_test.values)), 'BernoulliNB': (np.mean(predictions_three == y_test.values))}\n",
    "acc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results I can say that the BernoulliNB classifier is the better choice for this set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Where I could go next\n",
    "There are other areas that I could explore next if I were to continue. I could tweak the parameters even more on both the BernoulliNB and MultinomialNB models, or possibly try other machine learning models away from the Naive Bayes library. I could also analyse the predictions to see if there was a trend in the reviews that were misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Apply the model to test data\n",
    "Now that I have created my model I will apply it to my test dataset and export the results as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_predictions = pipeline.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['classification'] = test_data_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't care what anyone says, I like Hillary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harvard is dumb, i mean they really have to be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harvard is for dumb people.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  classification\n",
       "0   I don't care what anyone says, I like Hillary...               0\n",
       "1  harvard is dumb, i mean they really have to be...               1\n",
       "2                   I'm loving Shanghai > > > ^ _ ^.               1\n",
       "3                        harvard is for dumb people.               1\n",
       "4  As i stepped out of my beautiful Toyota, i hea...               0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('test_data_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.0 Real-world applications\n",
    "This has been an interesting model to work on, but what are the practical applications of a model like this? Here are a couple of areas where I believe we could apply it:\n",
    "\n",
    "1. This model doesn't have to be limited to movie reviews. This could be inbuilt into a chatbot system to determine the sentiment of a conversation between a customer and a customer service representative. \n",
    "2. Internal business communication could be anonymously fed into the model to track the overall mood within the business. If there is a long trend of negative sentiment within the business, this could then be addressed to avoid long term issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
